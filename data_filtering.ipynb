{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td9XfqRerGUK",
        "outputId": "f6069e66-8ff0-4959-f806-4d812776ec98"
      },
      "outputs": [],
      "source": [
        "!pip install tld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NyGtk-2j0BH",
        "outputId": "ce7b2f4c-35ba-4baf-ebac-e369bbfa5ca0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import bz2\n",
        "import json\n",
        "\n",
        "from tld import get_tld\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdgmD6Kgkhs0"
      },
      "outputs": [],
      "source": [
        "#PATH_ROOT = '/content/drive/MyDrive/ADA'\n",
        "PATH_ROOT = ''\n",
        "PATH_PARQUET = PATH_ROOT + '/project_datasets'\n",
        "PATH_QUOTEBANK = PATH_ROOT + 'Quotebank'\n",
        "PATH_TO_QUOTES = PATH_QUOTEBANK + '/quotes-{year}.json.bz2'\n",
        "PATH_TO_WORDS = PATH_ROOT + 'Data/environment_keywords.txt'\n",
        "PATH_TO_OUT1 = PATH_ROOT + 'Data/quotes-{year}-labeled.json.bz2'\n",
        "PATH_TO_OUT2 = PATH_ROOT + 'Data/quotes-{year}-filtered.json.bz2'\n",
        "PATH_TO_OUT = PATH_ROOT + 'Data/time_series_{year}.json.bz2'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOexZqhwjzCe"
      },
      "outputs": [],
      "source": [
        "def get_domain(url):\n",
        "    res = get_tld(url, as_object=True)\n",
        "    return res.domain\n",
        "\n",
        "\n",
        "def labeled_data(years, topic_name):\n",
        "    '''\n",
        "    Creates the labeled and filtered (only environment related) dataset.\n",
        "    :years: List with all the valid years for the articles (the ones published on any other date will be ignored).\n",
        "    :return: None.\n",
        "    '''\n",
        "    # Reading txt file with topic related words -> 1 word/string per line\n",
        "    with open(PATH_TO_WORDS) as file:\n",
        "        lines = file.readlines()\n",
        "        lines = [line.rstrip() for line in lines]\n",
        "\n",
        "\n",
        "    for year in years:\n",
        "        with bz2.open(PATH_TO_QUOTES.format(year=year), 'rb') as s_file:\n",
        "            with bz2.open(PATH_TO_OUT1.format(year=year), 'xb') as d_file1, bz2.open(PATH_TO_OUT2.format(year=year), 'xb') as d_file2:\n",
        "                for instance in s_file:\n",
        "\n",
        "                    instance = json.loads(instance)                                 # loading a sample\n",
        "\n",
        "\n",
        "                    urls = instance['urls']                                         # Extracting list of links\n",
        "                    domains = []\n",
        "                    for url in urls:\n",
        "                        tld = get_domain(url)\n",
        "                        domains.append(tld)\n",
        "                    instance['domains'] = domains                                   # Updating the sample with domain name    \n",
        "                    del instance['phase']                                           # Drop phase column since it won't be used.\n",
        "\n",
        "\n",
        "                    # Adding the label column\n",
        "                    column_name = topic_name + \"_related\"\n",
        "                    if any(word in instance['quotation'] for word in lines):\n",
        "                        instance[column_name] = 1\n",
        "                        d_file2.write((json.dumps(instance)+'\\n').encode('utf-8'))  # Save filtered dataset (only environment related).\n",
        "                    else:\n",
        "                        instance[column_name] = 0\n",
        "\n",
        "\n",
        "                    d_file1.write((json.dumps(instance)+'\\n').encode('utf-8'))      # Save labeled dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labeled_data([2018], 'Enviroment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labeled_data([2019], 'Enviroment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labeled_data([2020], 'Enviroment')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def time_analysis_data(years, newspapers):\n",
        "    '''\n",
        "    Creates the labeled and filtered (only environment related) dataset.\n",
        "    :years: List with all the valid years for the articles (the ones published on any other date will be ignored).\n",
        "    :newspapers: Lis with all the newspapers (only quotes cited by any of these newspapers will be used).\n",
        "    :return: None.\n",
        "    '''\n",
        "\n",
        "    for year in years:\n",
        "        with bz2.open(PATH_TO_OUT2.format(year=year), 'rb') as s_file:\n",
        "            with bz2.open(PATH_TO_OUT.format(year=year), 'xb') as d_file:\n",
        "                for instance in s_file:\n",
        "\n",
        "                    instance = json.loads(instance) # loading a sample\n",
        "                    \n",
        "                    for domain in instance['domains']:\n",
        "                        if any(newspaper in domain for newspaper in newspapers):\n",
        "                            new_instance = {'date': instance['date'], 'newspaper': domain}\n",
        "                            d_file.write((json.dumps(new_instance)+'\\n').encode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_analysis_data([2018], ['nytimes', 'washingtonpost', 'theguardian', 'wsj', 'bloomberg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_analysis_data([2019], ['nytimes', 'washingtonpost', 'theguardian', 'wsj', 'bloomberg'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_analysis_data([2020], ['nytimes', 'washingtonpost', 'theguardian', 'wsj', 'bloomberg'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
